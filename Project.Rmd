---
title: "Practical Machine Learning Project"
author: "Jerry"
date: "September 22, 2015"
output: html_document
---
# Abstract

In this project we create model which analyzes the performance of 5 accelerometers attached to test subjects performing exercises.  Using the performance data the model classifies how well the exercises were performed into 5 "classe" factor values. 

To create the model we clean and filter the data to remove non relavant and partial
data. A Random Forest, with K-folder (n=10) cross validation, is used to calculate the model based on a 60% subset of the training data. The produced model shows low OOB error rate of 0.98%. This model is further validate by using the remaining 40% of the training data as a Validation set. Against this Validation set the model showed good accuracy, >98%, in predicting the correct classe values. 

Finally, the model is used to predict classe for the 20 items in the Test set.  For the test set the model correctly predicted the classe value 100% of the time.  

# Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

# Analysis

## Load required packages and set flags
```{r init - Load packages and set flags, message = FALSE, warning=FALSE}
require(data.table)
require(caret)
require(e1071)
require(randomForest)
require(parallel)
require(doParallel)

# set this to TRUE to reuse the model from the last run.
# This will greatly improves (i.e. removes 98% of the time) the speed the run.
USE_PREVIOUS_MODEL <- FALSE

```


## Load data files
Read in the files.  Convert excel div 0, nulls, and null string to NAs


```{r Load datafiles }
setInternet2(TRUE)

url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
origTraining <- fread(url, na.strings = c("NA","#DIV/0!", ""))

url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
origTest <- fread(url, na.strings = c("NA","#DIV/0!", ""))
```

## Data Cleaning and Filtering

We need to clean and filter data before we can use it.  As pointed out in the lectures this one of the  
critical steps in the processing.

### Determine the predictors to use

We want to filter out data to only the parsimonious predictors that matter.
As specified in the instructions, we only want "belt, arm, forearm, or dumbell" columns.  And after
several tests we determine that we should use only the columns with data.  That is, no NAs.

```{r eliminate predictors with NA or "" values}
isAnyMissing <- sapply(origTest, function (x) any(is.na(x) | x == ""))
# we only care about data for the forearm, arms, dumbell, and belt and those that have values
isPredictor <- !isAnyMissing & grepl("belt|[^(fore)]arm|dumbbell|forearm", names(isAnyMissing))
predictors <- names(isAnyMissing)[isPredictor]
```

### Filter Training dataset to Only Predictor and classe columns
We subset the columns in the data sets to only include the prediction columns from above and the "classe" column.  
Also, since classe has a discrete set of values we make the classe column a factor.
```{r subset training columns }
# use the classe + the predictor columns as a filter
colsToUse <- c("classe", predictors)
# Create filtered training subset containing only the columns from the training file we chose above
filteredTraining <- origTraining[, colsToUse, with=FALSE]
# make classe a factor
filteredTraining <- filteredTraining[, classe := factor(filteredTraining[, classe])]
```

##  Split Filtered Training into Training/Validation datasets

From the entire filtered training data we split the data into a training and a validation datasets
We will use the training dataset to train our model and the Validation dataset to validate the
model.

60/40 ratio was determined by a reading posting on stackexchange.com and other site.

```{r split into training and test datasets}
# set seed.  Note: the seed can cause small shifts in submitted answers
set.seed(160615)   
# partition the test data in to test and validation sets
inTrain <- createDataPartition(filteredTraining$classe, p=0.60, list=FALSE)
Train <- filteredTraining[inTrain,]
Validation <- filteredTraining[-inTrain,]
```

## Pre-process training dataset
```{r Preprocess training dataset}
trainPred <- Train[, predictors, with=FALSE]
# center and scale the data.  I am not sure if this required for Random Forest
preProc <- preProcess(trainPred, method=c("center", "scale"))
tempCentScale <- predict(preProc, trainPred)
trainCentScale <- data.table(data.frame(classe = Train[, classe], tempCentScale))
```

## Pre-process Validation dataset
```{r preprocess test dataset}
validationPred <- Validation[, predictors, with=FALSE]
# center and scale the data
preProc <- preProcess(validationPred, method=c("center", "scale"))
tempCentScale <- predict(preProc, validationPred)
validationCentScale <- data.table(data.frame(classe = Validation[, classe], tempCentScale))
```

## Check for constant predictors

Check to see if there are predictors with near 0 variance (constant) indicating 
they are un-informative.  We remove them since they would likely break the tree prediction or at least skew the model.   A quick review of the data shows that we should not have any of these.  If we do I want to know about it and see if I made an error in my understanding of the data. 
See http://www.r-bloggers.com/near-zero-variance-predictors-should-we-remove-them/ for more information.
```{r check near 0 variance}
nzvPredictors <- nearZeroVar(trainCentScale, saveMetrics=TRUE)
if (any(nzvPredictors$nzv)) {
  message(paste("ERROR - near zero values in Predictors.  Check these ", nzvPredictors$nzv)) 
} else  {
  message("No near 0 variance predictors.  Good to go!!")
}
```


## Train the model

Now that the data has been preprocessed we can train the model.  We are using the Random Forest model since the values of classe are discrete, and as pointed out in the lectures, the Random Forest accuracy is high. We further try to improve the accuracy by use a 10 fold cross validation.  

Note: Also pointed out in the lectures was that the Random Forest can be a bit slow.  To combat this we employ 2 tactics.  First, we use parallel clustering to take advantage of multiple processing cores available on many CPUs.  Even with this, it takes 5 minutes on a i7-4970K system with 16 GB of memory to train the model. Often we re-run the model generation not because the model has changed, but because the report content or format has changed.  Therefore we 
have a second tactic that allow reusing the previously generated model.

```{r Train Model}

trainingFile <- "trainingModel.Rdata"
if ( !USE_PREVIOUS_MODEL ) {
  # If here, generate a new model
  
  # Enable parallel processing to make training faster
  require(parallel)
  require(doParallel)
  cl <- makeCluster(detectCores() - 1)
  registerDoParallel(cl)
  
  
  # Train the model, cross-validate with 
   trainingModel  <- train(classe ~ ., data=trainCentScale,
              trControl=trainControl(method="cv",number=10), method="rf")
  # Terminate parallel processing
  stopCluster(cl)
  # Save the training model to disk
  save(trainingModel, file=trainingFile)
} else {
  #Restore training model from disk
  load(trainingFile)
}

# see the results of k-fold cross-validation in the training model
print (trainingModel)
```

### Tree accuracy

We see that with k-fold (k=10) cross-validation we achieve an accuracy of 0.99 or 99%.  This accuracy was achieved when the number of variables per level (mtry) in the model was 27.


## OOB errors
As a check lets take a look at the model's OOB estimate of error for our classe values shown in Figure 1.  We see 
that the OOB (out of sample) error rate is low, 0.84%, increasing our confidence in the model.
 

## Evaluating the model on Validation set

We validate the model against our 40% of the training data we used to create a validation set.
We should see good accuracy, and and similar prevelence and balance amoung the classe entries.

```{r evaluate model vs validation set }
hatValidation <- predict(trainingModel, validationCentScale)
cmValidation <- confusionMatrix(hatValidation, validationCentScale[, classe])
cmValidation
```
We get an accuracy of `r cmValidation$overall['Accuracy']`.  Also, we see the classe counts are relatively balanced, so this accuracy is likely real.

The need for a balanced distribution was pointed out in the answer to this question, http://stat.stackexchange.com/questions/30691/how-to-interpret-oob-and-confusion-matrix-for-random-forest.  The 3rd answer
begins with " Your set is sharply unbalanced -- RF usually fails in this scenario (i.e. predicts well only the bigger class)"

Checking the Sensitivity (True Postive Rate) and Specificity (True Negative Rate) for the classes we see these values are high so this helps confirm that the model does indeed correctly predict classe. 


## Predict classe of the Test set 

We can now run the prediction with our model to obtain the classe values which will
be evaluated by the Coursera grader.

```{r Predict with test data}
testPred <- predict(preProc, origTest[, predictors, with=FALSE])
testHat <- predict(trainingModel, testPred)
```

### Create answer files
Using the function provided in the instructions, we create the answer files to be submitted to Coursera.

```{r write answer files}
# Write submission answer files to send to Coursera using the provided function
pml_write_files = function(x){
  n = length(x)
  path <- "./answers"
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=file.path(path, filename),quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(testHat)
```

## Answer file correctness (ie. Does it work?)

The created answer files were accepted by the Coursera grader. 20 of 20 answers were graded as correct!




# Appendix

## Model information

### Figure 1. Final Model Summary
``` {r echo=FALSE}
trainingModel$finalModel
```

